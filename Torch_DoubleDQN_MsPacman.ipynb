{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4259548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\torch_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import statistics\n",
    "\n",
    "# Preprocess each frame from 210x160 to 84x84\n",
    "class Preprocess(gym.ObservationWrapper):\n",
    "    \n",
    "    def __init__(self, env=None):\n",
    "        super(Preprocess, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return Preprocess.process(obs)\n",
    "\n",
    "    def process(frame):\n",
    "        frame = np.reshape(frame, [210, 160, 1]).astype(np.uint8)\n",
    "        resized_frame = cv2.resize(frame, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        result = resized_frame[18:102, :]\n",
    "        result = np.reshape(result, [84, 84, 1])\n",
    "        return result.astype(np.uint8)\n",
    "\n",
    "# Change axis accordingly to neural network input\n",
    "class ChangeAxis(gym.ObservationWrapper):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super(ChangeAxis, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eab2c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolutional neural network\n",
    "class DQN_Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN_Network, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        output_size = self.get_output_size(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(output_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def get_output_size(self, input_shape):\n",
    "        ot = self.conv(torch.zeros(1, *input_shape))\n",
    "        return int(np.prod(ot.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255.0\n",
    "        mid_output = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(mid_output)\n",
    "\n",
    "#DQN agent \n",
    "class DQN_Agent:\n",
    "\n",
    "    def __init__(self, state_space, action_space, epsilon_max, epsilon_min, rb_size, batch_size, gamma, lr):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space        \n",
    "       \n",
    "        self.dqn = DQN_Network(state_space, action_space).to(self.device)\n",
    "        self.dqn_target = DQN_Network(state_space, action_space).to(self.device)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "        self.l1 = nn.SmoothL1Loss().to(self.device)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.epsilon = epsilon_max\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = (epsilon_max - epsilon_min)/20000\n",
    "\n",
    "        self.mem_size = rb_size\n",
    "        self.state_buffer = torch.zeros(self.mem_size, *self.state_space, dtype=torch.uint8)\n",
    "        self.action_buffer = torch.zeros(self.mem_size, 1)\n",
    "        self.reward_buffer = torch.zeros(self.mem_size, 1)\n",
    "        self.next_state_buffer = torch.zeros(self.mem_size, *self.state_space, dtype=torch.uint8)\n",
    "        self.done_buffer = torch.zeros(self.mem_size, 1)\n",
    "        self.last_pos = 0\n",
    "        self.current = 0\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:  \n",
    "            return torch.tensor([[random.randrange(self.action_space)]])\n",
    "        else:\n",
    "            return torch.argmax(self.dqn(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
    "    \n",
    "    def train(self):\n",
    "        if self.batch_size > self.current:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.sample()\n",
    "        states = states.float().to(self.device)\n",
    "        actions = actions.float().to(self.device)\n",
    "        rewards = rewards.float().to(self.device)\n",
    "        next_states = next_states.float().to(self.device)\n",
    "        dones = dones.float().to(self.device)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        current = self.dqn(states).gather(1, actions.long())\n",
    "        target = rewards + torch.mul(self.gamma * self.dqn_target(next_states).gather(1,torch.argmax(self.dqn(next_states), dim=1).unsqueeze(1)), 1 - dones)\n",
    "    \n",
    "        loss = self.l1(current, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.epsilon -= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "        \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.state_buffer[self.last_pos] = state\n",
    "        self.action_buffer[self.last_pos] = action\n",
    "        self.reward_buffer[self.last_pos] = reward\n",
    "        self.next_state_buffer[self.last_pos] = next_state\n",
    "        self.done_buffer[self.last_pos] = done\n",
    "        \n",
    "        self.last_pos = (self.last_pos + 1) % self.mem_size\n",
    "        self.current = min(self.current + 1, self.mem_size)\n",
    "    \n",
    "    def sample(self):\n",
    "        indices = random.choices(range(self.current), k=self.batch_size)\n",
    "        states = self.state_buffer[indices]\n",
    "        actions = self.action_buffer[indices]\n",
    "        rewards = self.reward_buffer[indices]\n",
    "        next_states = self.next_state_buffer[indices]\n",
    "        dones = self.done_buffer[indices]      \n",
    "        return states, actions, rewards, next_states, dones\n",
    "        \n",
    "    def update_target(self):\n",
    "         self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "            \n",
    "    def save_models(self):\n",
    "        torch.save(self.dqn.state_dict(), \"double_state_dict_model.pt\")\n",
    "        torch.save(self.dqn_target.state_dict(), \"double_state_dict_target_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe10d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train run\n",
    "def train_agent(episodes=2000):\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "   \n",
    "    env = gym.make('ALE/MsPacman-v5', frameskip=16, obs_type='grayscale')\n",
    "    env.seed(12)\n",
    "    env.action_space.seed(12)\n",
    "    env = Preprocess(env)\n",
    "    env = ChangeAxis(env) \n",
    "     \n",
    "    state_space = env.observation_space.shape\n",
    "    action_space = env.action_space.n\n",
    "    agent = DQN_Agent(state_space=state_space, action_space=action_space, epsilon_max=1.0, epsilon_min=0.02, rb_size=200000, batch_size=32, gamma=0.95, lr=0.00025)\n",
    "    \n",
    "    update_target = 200\n",
    "    total_steps = 0\n",
    "          \n",
    "    for episode in range(episodes):\n",
    "        episode_score = 0\n",
    "        step = 0\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = torch.from_numpy(np.array(state)).unsqueeze(0)\n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            step += 1\n",
    "            \n",
    "            next_state, reward, done, info = env.step(int(action[0]))\n",
    "            next_state = torch.from_numpy(np.array(next_state)).unsqueeze(0)\n",
    "            episode_score += reward  \n",
    "                    \n",
    "            agent.store(state, action, reward, next_state, done)\n",
    "            agent.train()\n",
    "            if step % update_target == 0:\n",
    "                agent.update_target()\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                total_steps += step\n",
    "                writer.add_scalar(\"Score/Step\", episode_score, total_steps)\n",
    "                writer.add_scalar(\"Score/Episode\", episode_score, episode)\n",
    "                writer.flush()\n",
    "                break        \n",
    "        \n",
    "    agent.save_models()\n",
    "    env.close()\n",
    "    print(\"Done\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc0a416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\torch_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_agent(episodes=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c584b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent evaluation after training\n",
    "def evaluate():\n",
    "    env = gym.make('ALE/MsPacman-v5', frameskip=16, obs_type='grayscale')\n",
    "    env = Preprocess(env)\n",
    "    env = ChangeAxis(env) \n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model = DQN_Network(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    model.load_state_dict(torch.load(\"double_state_dict_model.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    epsilon = 0.02\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(30):\n",
    "        episode_score = 0\n",
    "        state = env.reset()\n",
    "        state = torch.from_numpy(np.array(state)).unsqueeze(0)\n",
    "    \n",
    "        while True:\n",
    "            if random.random() < epsilon:  \n",
    "                action =  torch.tensor([[random.randrange(9)]])\n",
    "            else:\n",
    "                action = torch.argmax(model(state.to(device))).unsqueeze(0).unsqueeze(0).cpu()\n",
    "            \n",
    "            next_state, reward, done, info = env.step(int(action[0]))\n",
    "            next_state = torch.from_numpy(np.array(next_state)).unsqueeze(0)\n",
    "            episode_score += reward  \n",
    "        \n",
    "            state = next_state\n",
    "        \n",
    "            if done:\n",
    "                scores.append(episode_score)\n",
    "                print(\"Episode: {}, Score: {}\" .format(i + 1, scores[-1]))\n",
    "                break\n",
    "            \n",
    "    print(\"Avarage score: {}\" .format(np.mean(scores)))\n",
    "    print(statistics.stdev(scores))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d197a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\torch_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Score: 4800.0\n",
      "Episode: 2, Score: 2850.0\n",
      "Episode: 3, Score: 4520.0\n",
      "Episode: 4, Score: 1250.0\n",
      "Episode: 5, Score: 5040.0\n",
      "Episode: 6, Score: 2820.0\n",
      "Episode: 7, Score: 5660.0\n",
      "Episode: 8, Score: 3480.0\n",
      "Episode: 9, Score: 4970.0\n",
      "Episode: 10, Score: 2390.0\n",
      "Episode: 11, Score: 1210.0\n",
      "Episode: 12, Score: 3050.0\n",
      "Episode: 13, Score: 3140.0\n",
      "Episode: 14, Score: 2040.0\n",
      "Episode: 15, Score: 1250.0\n",
      "Episode: 16, Score: 2130.0\n",
      "Episode: 17, Score: 780.0\n",
      "Episode: 18, Score: 700.0\n",
      "Episode: 19, Score: 3330.0\n",
      "Episode: 20, Score: 1720.0\n",
      "Episode: 21, Score: 2780.0\n",
      "Episode: 22, Score: 3100.0\n",
      "Episode: 23, Score: 4770.0\n",
      "Episode: 24, Score: 3370.0\n",
      "Episode: 25, Score: 1540.0\n",
      "Episode: 26, Score: 4470.0\n",
      "Episode: 27, Score: 2940.0\n",
      "Episode: 28, Score: 5100.0\n",
      "Episode: 29, Score: 4960.0\n",
      "Episode: 30, Score: 2420.0\n",
      "Avarage score: 3086.0\n",
      "1442.12152149485\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94afc53f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_env]",
   "language": "python",
   "name": "conda-env-torch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
